(function(){"use strict";self.exports={},importScripts("/genai_bundle.js");const{FilesetResolver:y,LlmInference:_}=self.exports,x="https://cdn.jsdelivr.net/npm/@mediapipe/tasks-genai@0.10.25/wasm";let a=null,i=null;const w=async s=>{const{modelBlob:t,modelSource:e,options:r}=s;a&&(await a.close(),a=null);try{if(!("gpu"in navigator))throw new Error("WebGPU is not supported.");if(!t)throw new Error(`Model data for ${e} not found.`);const o=URL.createObjectURL(t),n=await y.forGenAiTasks(x),{maxTokens:l=4096,topK:p=40,temperature:c=.3,randomSeed:d=101,supportAudio:g=!1,maxNumImages:f=1}=r;a=await _.createFromOptions(n,{baseOptions:{modelAssetPath:o,delegate:"GPU"},maxTokens:l,topK:p,temperature:c,randomSeed:d,supportAudio:g,maxNumImages:f}),URL.revokeObjectURL(o),self.postMessage({type:"init_done",payload:{modelIdentifier:`${e}-${JSON.stringify(r)}`}})}catch(o){a=null;const n=o instanceof Error?o.message:"Unknown initialization error.";self.postMessage({type:"init_error",payload:{error:n}})}},b=async()=>{a&&(await a.close(),a=null),self.postMessage({type:"unload_done"})},u=(s,t,e,r)=>{if(!a)throw new Error("Offline model is not initialized.");i=new AbortController;const o=i.signal;return new Promise((n,l)=>{const p=()=>l(new DOMException("Translation cancelled.","AbortError"));o.addEventListener("abort",p,{once:!0});try{let c="";const d=(m,k)=>{o.aborted||(c+=m,r&&self.postMessage({type:"translation_chunk",payload:{chunk:m}}),k&&(o.removeEventListener("abort",p),n(c.trim())))},f=`Translate the following ${t==="Auto Detect"?"auto-detect the source language":`from ${t}`} text into concise ${e}: "${s}". 
 Provide *only* the translated text. Do not include any additional explanations, commentary, or greetings.`;a.generateResponse(f,d)}catch(c){o.removeEventListener("abort",p),l(c)}})},h=async s=>{if(!a)return self.postMessage({type:"extract_text_error",payload:{error:"Offline model not initialized."}});const t="Extract all text from the following image. Return only the extracted text without any extra comments or explanations.";try{const e=await a.generateResponse([`<start_of_turn>user
${t}
`,{imageSource:s.imageUrl},`<end_of_turn>
<start_of_turn>model
`]);self.postMessage({type:"extract_text_done",payload:{text:e.trim()}})}catch(e){const r=e instanceof Error?e.message:"Offline image text extraction failed.";self.postMessage({type:"extract_text_error",payload:{error:r}})}},M=async s=>{if(!a)return self.postMessage({type:"transcribe_error",payload:{error:"Offline model not initialized."}});const{audioUrl:t,sourceLang:e}=s,o=`Transcribe the following audio. ${e==="Auto Detect"?"Auto-detect the language.":`The language is ${e}.`}  Return only the transcribed text.`;try{const n=await a.generateResponse([`<start_of_turn>user
 ${o} <end_of_turn>
<start_of_turn>model
`,{audioSource:t}]);self.postMessage({type:"transcribe_done",payload:{text:n.trim(),audioUrl:t}})}catch(n){const l=n instanceof Error?n.message:"Offline audio transcription failed.";self.postMessage({type:"transcribe_error",payload:{error:l,audioUrl:t}})}};self.onmessage=async s=>{const{type:t,payload:e}=s.data;try{switch(t){case"init":await w(e);break;case"unload":await b();break;case"translate_stream":{const r=await u(e.text,e.sourceLang,e.targetLang,!0);self.postMessage({type:"translation_done",payload:{result:r}});break}case"translate_full":{const r=await u(e.text,e.sourceLang,e.targetLang,!1);self.postMessage({type:"translation_full_done",payload:{result:r,originalPayload:e}});break}case"cancel_task":i==null||i.abort();break;case"extractText":await h(e);break;case"transcribe":await M(e);break;default:console.warn(`Unknown inference worker message type: ${t}`)}}catch(r){const o=r instanceof DOMException&&r.name==="AbortError",n=t.replace("_stream","").replace("_full","");if(o)self.postMessage({type:`${n}_cancelled`});else{const l=r instanceof Error?r.message:`Unknown error in ${t}.`;self.postMessage({type:`${n}_error`,payload:{error:l}})}}finally{i=null}}})();
