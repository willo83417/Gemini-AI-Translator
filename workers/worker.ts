// workers/worker.ts

	/**
	 * Environment Polyfill - MUST be at the very top.
	 * This spoofs the environment for MediaPipe's internal checks before the script is loaded.
	 */
	self.exports = {};
	//importScripts("/public/genai_bundle.js");//Development and Testing
	//importScripts(`${import.meta.env.BASE_URL}genai_bundle.js`); //yarn build is used for packaging.; yarn build 打包用(Backup-2)
	//const { FilesetResolver, LlmInference } = self.exports;
	import { FilesetResolver, LlmInference } from '@mediapipe/tasks-genai'; //yarn build is used for packaging.; yarn build 打包用?
	 
	if (typeof (self as any).HTMLImageElement === 'undefined') {
		(self as any).HTMLImageElement = class HTMLImageElement {};
	}
	if (typeof (self as any).Image === 'undefined') {
		(self as any).Image = (self as any).HTMLImageElement;
	}
	if (typeof (self as any).HTMLVideoElement === 'undefined') {
		(self as any).HTMLVideoElement = class HTMLVideoElement {};
	}
	
	/**
	 * Custom AudioContext Polyfill for Web Workers.
	 * The MediaPipe GenAI library requires `AudioContext.decodeAudioData` to process audio,
	 * but this API is not available in workers. This polyfill provides the necessary functionality.
	 */
	if (typeof (self as any).AudioContext === 'undefined') {
		// FIX: Define a helper function to read strings from a DataView, replacing the non-standard prototype extension.
		const dataViewToString = (view: DataView, offset: number, length: number): string => {
			let result = '';
			for (let i = 0; i < length; i++) {
				result += String.fromCharCode(view.getUint8(offset + i));
			}
			return result;
		};

		class PolyfillAudioBuffer {
			readonly sampleRate: number;
			readonly length: number;
			readonly duration: number;
			readonly numberOfChannels: number;
			private channels: Float32Array[];
	
			constructor(options: {
				sampleRate: number;
				numberOfChannels: number;
				pcmData: Float32Array[];
			}) {
				this.sampleRate = options.sampleRate;
				this.numberOfChannels = options.numberOfChannels;
				this.channels = options.pcmData;
				this.length = this.channels[0]?.length || 0;
				this.duration = this.length / this.sampleRate;
			}
	
			getChannelData(channel: number): Float32Array {
				if (channel >= this.numberOfChannels) {
					throw new Error(`Invalid channel index ${channel}.`);
				}
				return this.channels[channel];
			}
		}
	
		(self as any).AudioContext = class PolyfillAudioContext {
			// A lightweight WAV parser to mimic decodeAudioData
			decodeAudioData(arrayBuffer: ArrayBuffer): Promise<PolyfillAudioBuffer> {
				return new Promise((resolve, reject) => {
					try {
						const view = new DataView(arrayBuffer);
						
						// Basic WAV header validation
						// FIX: Use the helper function instead of the non-standard prototype method.
						if (dataViewToString(view, 0, 4) !== 'RIFF' || dataViewToString(view, 8, 4) !== 'WAVE') {
							return reject(new DOMException('Invalid WAV file header', 'DataCloneError'));
						}
	
						// Find 'fmt ' and 'data' chunks
						let fmtChunkOffset = -1, dataChunkOffset = -1, dataChunkSize = 0;
						let offset = 12;
						while(offset < view.byteLength) {
							// FIX: Use the helper function instead of the non-standard prototype method.
							const chunkId = dataViewToString(view, offset, 4);
							const chunkSize = view.getUint32(offset + 4, true);
							if (chunkId === 'fmt ') {
								fmtChunkOffset = offset + 8;
							} else if (chunkId === 'data') {
								dataChunkOffset = offset + 8;
								dataChunkSize = chunkSize;
							}
							offset += 8 + chunkSize;
						}
	
						if (fmtChunkOffset === -1 || dataChunkOffset === -1) {
							return reject(new DOMException('Could not find fmt and data chunks in WAV file', 'DataCloneError'));
						}
	
						const numChannels = view.getUint16(fmtChunkOffset + 2, true);
						const sampleRate = view.getUint32(fmtChunkOffset + 4, true);
						const bitsPerSample = view.getUint16(fmtChunkOffset + 14, true);
	
						if (bitsPerSample !== 16) {
							return reject(new DOMException(`Unsupported bitsPerSample: ${bitsPerSample}. Only 16-bit PCM is supported.`, 'NotSupportedError'));
						}
	
						const pcmData: Float32Array[] = Array.from({ length: numChannels }, () => new Float32Array(dataChunkSize / (numChannels * (bitsPerSample / 8))));
						const samplesCount = dataChunkSize / (bitsPerSample / 8);
	
						for (let i = 0; i < samplesCount; i++) {
							const sampleOffset = dataChunkOffset + i * (bitsPerSample / 8);
							const channel = i % numChannels;
							const sampleIndex = Math.floor(i / numChannels);
							
							const value = view.getInt16(sampleOffset, true);
							pcmData[channel][sampleIndex] = value / 32768.0; // Normalize to [-1.0, 1.0]
						}
	
						resolve(new PolyfillAudioBuffer({
							sampleRate,
							numberOfChannels: numChannels,
							pcmData,
						}));
					} catch (e) {
						reject(e);
					}
				});
			}
		};
	}
	
	if (typeof (self as any).OfflineAudioContext === 'undefined') {
		(self as any).OfflineAudioContext = (self as any).AudioContext;
	}

	// Per user request, load the library using importScripts for a classic worker environment.
	//self.exports = {};
	//importScripts("/public/genai_bundle.js"); //Development and Testing;開發測試
	//importScripts(`${import.meta.env.BASE_URL}genai_bundle.js`); //yarn build is used for packaging.; yarn build 打包用
	//const { FilesetResolver, LlmInference } = self.exports;

	const MEDIAPIPE_WASM = "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-genai@0.10.25/wasm";

	let llmInference: InstanceType<typeof LlmInference> | null = null;
	let currentTaskAbortController: AbortController | null = null;

const handleInit = async (payload: any) => {
    const { modelBlob, modelSource, options } = payload;
    
    if (llmInference) {
        await llmInference.close();
        llmInference = null;
    }

    try {
        if (!('gpu' in navigator)) {
            throw new Error('WebGPU is not supported.');
        }

        if (!modelBlob) {
            throw new Error(`Model data for ${modelSource} not found.`);
        }
        
        const modelUrl = URL.createObjectURL(modelBlob);
        const filesetResolver = await FilesetResolver.forGenAiTasks(MEDIAPIPE_WASM);

        const { 
            maxTokens = 4096, topK = 40, temperature = 0.3, randomSeed = 101, supportAudio = false, maxNumImages = 1
        } = options;
        
        llmInference = await LlmInference.createFromOptions(filesetResolver, {
            baseOptions: { modelAssetPath: modelUrl, delegate: 'GPU' },
            maxTokens, topK, temperature, randomSeed, supportAudio, maxNumImages,
        });
        
        URL.revokeObjectURL(modelUrl);
        self.postMessage({ type: 'init_done', payload: { modelIdentifier: modelSource } });

    } catch (error) {
        llmInference = null;
        const errorMessage = error instanceof Error ? error.message : 'Unknown initialization error.';
        self.postMessage({ type: 'init_error', payload: { error: errorMessage } });
    }
};

const handleUnload = async () => {
    if (llmInference) {
        console.log('Unloading offline model from inference worker...');
        await llmInference.close();
        llmInference = null;
        console.log('Offline model unloaded successfully.');
    }
    self.postMessage({ type: 'unload_done' });
};

const performTranslation = (text: string, sourceLang: string, targetLang: string, stream: boolean) => {
    if (!llmInference) throw new Error('Offline model is not initialized.');
    
    currentTaskAbortController = new AbortController();
    const signal = currentTaskAbortController.signal;

    return new Promise((resolve, reject) => {
        const handleAbort = () => reject(new DOMException('Translation cancelled.', 'AbortError'));
        signal.addEventListener('abort', handleAbort, { once: true });
        
        try {
            let fullText = "";
            const streamCallback = (partialResult: string, done: boolean) => {
                if (signal.aborted) return;
                fullText += partialResult;
                if (stream) {
                    self.postMessage({ type: 'translation_chunk', payload: { chunk: partialResult } });
                }
                if (done) {
                    signal.removeEventListener('abort', handleAbort);
                    resolve(fullText.trim());
                }
            };

            const sourceInstruction = sourceLang === 'Auto Detect' 
                ? 'auto-detect the source language'
                : `from ${sourceLang}`;
            const prompt = `Translate the following ${sourceInstruction} text into concise ${targetLang}: "${text}". \n Provide *only* the translated text. Do not include any additional explanations, commentary, or greetings.`;

            llmInference.generateResponse(prompt, streamCallback);
        } catch (error) {
            signal.removeEventListener('abort', handleAbort);
            reject(error);
        }
    });
};

const handleExtractText = async (payload: any) => {
    if (!llmInference) {
        return self.postMessage({ type: 'extract_text_error', payload: { error: 'Offline model not initialized.' } });
    }
    
    const { imageBitmap } = payload;
    // Gemma-3 Specific: Use <image> token in the prompt
    const prompt = `Extract all text from the following image. Return only the extracted text without any extra comments or explanations.`;
    try {
        const response = await llmInference.generateResponse([
            `<start_of_turn>user\n${prompt}\n`, 
            { imageSource: imageBitmap }, 
            `<end_of_turn>\n<start_of_turn>model\n`,
        ]);
        
        imageBitmap.close();
        self.postMessage({ type: 'extract_text_done', payload: { text: response.trim() } });
    } catch (error) {
        console.error('OCR Worker Error:', error);
        const errorMessage = error instanceof Error ? error.message : 'Offline image text extraction failed.';
        self.postMessage({ type: 'extract_text_error', payload: { error: errorMessage } });
    }
};

const handleTranscribe = async (payload: any) => {
    if (!llmInference) {
        return self.postMessage({ type: 'transcribe_error', payload: { error: 'Offline model not initialized.' } });
    }
    const { audioData, sourceLang } = payload; 
    
    let audioUrl: string | null = null;
    try {
        audioUrl = URL.createObjectURL(audioData);
        
        const languageClause = sourceLang === 'Auto Detect' ? 'Auto-detect the language.' : `The language is ${sourceLang}.`;
        

        const prompt = `Transcribe the following audio. ${languageClause}  Return only the transcribed text.`;
        
        const response = await llmInference.generateResponse([
            `<start_of_turn>user\n ${prompt} <end_of_turn>\n<start_of_turn>model\n`, 
            { audioSource: audioUrl }
        ]);
        
        if (!response || response.trim() === "") {
             console.warn("Model returned an empty response for transcription.");
        }

        self.postMessage({ type: 'transcribe_done', payload: { text: response.trim() } });
    } catch (error) {
        console.error('Transcription Internal Error:', error);
        const errorMessage = error instanceof Error ? error.message : 'Offline audio transcription failed.';
        self.postMessage({ type: 'transcribe_error', payload: { error: errorMessage } });
    } finally {
        if (audioUrl) {
            URL.revokeObjectURL(audioUrl);
        }
    }
};

self.onmessage = async (event: MessageEvent) => {
    const { type, payload } = event.data;
    try {
        switch (type) {
            case 'init':
                await handleInit(payload);
                break;
            case 'unload':
                await handleUnload();
                break;
            case 'translate_stream': {
                const result = await performTranslation(payload.text, payload.sourceLang, payload.targetLang, true);
                self.postMessage({ type: 'translation_done', payload: { result } });
                break;
            }
            case 'translate_full': {
                const result = await performTranslation(payload.text, payload.sourceLang, payload.targetLang, false);
                self.postMessage({ type: 'translation_full_done', payload: { result } });
                break;
            }
            case 'cancel_task':
                currentTaskAbortController?.abort();
                break;
            case 'extractText':
                await handleExtractText(payload);
                break;
            case 'transcribe':
                await handleTranscribe(payload);
                break;
            default:
                console.warn(`Unknown inference worker message type: ${type}`);
        }
    } catch (error) {
        const isAbort = error instanceof DOMException && error.name === 'AbortError';
        const baseType = type.replace('_stream', '').replace('_full', '');
        
        if (isAbort) {
            self.postMessage({ type: 'translation_cancelled' });
        } else {
            const errorMessage = error instanceof Error ? error.message : `Unknown error in ${type}.`;
            self.postMessage({ type: `${baseType}_error`, payload: { error: errorMessage } });
        }
    } finally {
        currentTaskAbortController = null;
    }
};
